
#load data sets

taz_subset <- read.csv("taz_subset (1).csv")
sueddeutsche_subset <- read.csv("sueddeutsche_subset (1).csv")

#--------------Create DTM for TAZ

options(stringsAsFactors = FALSE)
library(quanteda)

# read the corpus data
taz_corpus <- corpus(taz_subset$body, docnames = taz_subset$X)

# Preprocessing of the corpus (tokensization)
taz_tokens <- taz_corpus %>%
  tokens(remove_punct = FALSE, remove_numbers = TRUE, remove_symbols = FALSE) %>%
  tokens_tolower()

####!!tokens to lower makes it impoosible to find regexes charcterised by Capital letters, 


# Create DTM 
taz_DTM <- taz_tokens %>%
  dfm()


#define dictionairy with regex: include here the list with gender terms
mydict <- dictionary(list(gender=c("[Gg]endern","geschlecht"),
                          signs =c("\\*")))
dict_dtm <- dfm_lookup(taz_DTM,mydict,nomatch = "unmatched")

#sum up dfm for dict
term_in_docs_taz_dict <- colSums(dict_dtm)
#sum up dfm for all
term_in_docs_taz <- colSums(taz_DTM)

#------------#short excurs: create Frequency lists
# sum columns for word counts
freqs <- colSums(dict_dtm)
# get vocabulary vector
words <- colnames(dict_dtm)
# combine words and their frequencies in a data frame
wordlist <- data.frame(words, freqs)
# re-order the wordlist by decreasing frequency
wordIndexes <- order(wordlist[, "freqs"], decreasing = TRUE)
wordlist <- wordlist[wordIndexes, ]

#-----------------------------DTM for Sueddeutsche (here used as reference corpora)

# read the corpus data

sueddeutsche_corpus <- corpus(sueddeutsche_subset$body, docnames = sueddeutsche_subset$X)

# Preprocessing of the corpus
sueddeutsche_tokens <- sueddeutsche_corpus %>%
  tokens(remove_punct = FALSE, remove_numbers = TRUE, remove_symbols = FALSE) %>%
  tokens_tolower()

# Create DTM
sueddeutsche_DTM <- sueddeutsche_tokens %>%
  dfm()

#define dictionairy with regex
dict_dtm_sued <- dfm_lookup(sueddeutsche_DTM,mydict,nomatch = "unmatched")

#sum up tokens
term_in_docs_sueddeutsche_dict <- colSums(dict_dtm_sued)
term_in_docs_sueddeutsche <- colSums(sueddeutsche_DTM)


#---------------Log Likelihood function for a certain pattern (can be used to look up individual terms):


calculateLogLikelihood <- function(termCountsTarget, termCountsComparison, pattern, minSignificance = 6.63) {
  
  # Match regular expression pattern against term names
  patternMatch <- grep(pattern, names(termCountsTarget), value = TRUE)
  #this only matches the pattern and not a list
  
  # Only consider terms that match pattern in both corpora
  termsToCompare <- intersect(patternMatch, names(termCountsComparison))
  
  
  a <- termCountsTarget[termsToCompare]
  b <- termCountsComparison[termsToCompare]
  c <- sum(termCountsTarget)
  d <- sum(termCountsComparison)
  Expected1 = c * (a+b) / (c+d)
  Expected2 = d * (a+b) / (c+d)
  t1 <- a * log((a/Expected1) + (a == 0))
  t2 <- b * log((b/Expected2) + (b == 0))
  logLikelihood <- 2 * (t1 + t2)
  
  # compare relative frequencies to indicate over/underuse (we have to see whether we need that)
  relA <- a / c
  relB <- b / d
  # underused terms are multiplied by -1
  logLikelihood[relA < relB] <- logLikelihood[relA < relB] * -1
  
  logLikelihood[logLikelihood < minSignificance] <- 0#
  
  return(logLikelihood)
}


#---------------------test out


termCountsTarget <- term_in_docs_taz
termCountsComparison <- term_in_docs_sueddeutsche

logLikelihood <- calculateLogLikelihood(termCountsTarget, 
                                        termCountsComparison, 
                                        pattern = "\\*", 
                                        minSignificance = 6.63)
print(logLikelihood)

#----------------------------2. loglikelyhood 2 fÃ¼r das dict
calculateLogLikelihood2 <- function(termCountsTarget, termCountsComparison, minSignificance = 6.63) {  
  
  uniqueTerms <- setdiff(names(termCountsTarget), names(termCountsComparison))
  
  zeroCounts <- rep(0, length(uniqueTerms))
  names(zeroCounts) <- uniqueTerms
  termCountsComparison <- c(termCountsComparison, zeroCounts)
  
  termsToCompare <- intersect(names(termCountsTarget), names(termCountsComparison))
  
  a <- termCountsTarget[termsToCompare]
  b <- termCountsComparison[termsToCompare]
  c <- sum(termCountsTarget)
  d <- sum(termCountsComparison)
  Expected1 = c * (a+b) / (c+d)
  Expected2 = d * (a+b) / (c+d)
  t1 <- a * log((a/Expected1) + (a == 0))
  t2 <- b * log((b/Expected2) + (b == 0))
  logLikelihood <- 2 * (t1 + t2)
  
  # compare relative frequencies to indicate over/underuse
  relA <- a / c
  relB <- b / d
  # underused terms are multiplied by -1
  logLikelihood[relA < relB] <- logLikelihood[relA < relB] * -1
  
  logLikelihood[logLikelihood < minSignificance] <- 0
  
  return(logLikelihood)
}
#---------------------------------test loglikelyhood2

termCountsTarget <- (term_in_docs_taz_dict)
termCountsComparison <- (term_in_docs_sueddeutsche_dict)

logLikelihood2 <- calculateLogLikelihood2(termCountsTarget, 
                                        termCountsComparison, 
                                        minSignificance = 6.63)
